{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T-TmLFXbuvU1",
        "3r77GR9iu2Bv",
        "Qi2lm6J82cJ_",
        "eQaMCA1o75Zi",
        "8S0xmMQVG8Xc",
        "hwf1PdmYDFe3",
        "3md6QhWgIuwt",
        "wk8OVK4vKemG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "INPUT_LOC = \"/content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/\" # remmeber to add a \\ at the end\n",
        "OUTPUT_LOC = \"/content/drive/MyDrive/ma_dataset/similar_langs_small/100_set_auto/\" # remmeber to add a \\ at the end\n",
        "DATASET_PATH = INPUT_LOC # change this depending on the task\n",
        "JSON_PATH = OUTPUT_LOC+\"recordingslist.json\"\n",
        "LANGUAGES = 3"
      ],
      "metadata": {
        "id": "tO1e-rnjMM1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Imports**"
      ],
      "metadata": {
        "id": "T-TmLFXbuvU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "import argparse\n",
        "from itertools import compress\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader  \n",
        "from torch import optim\n",
        "# import torch\n",
        "# import torch.autograd as grad\n",
        "# import torch.nn.functional as F\n",
        "# import torch.nn as nn\n",
        "# from torch import optim\n",
        "# from torch.utils.data import DataLoader   \n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# from SpeechDataGenerator import SpeechDataGenerator\n",
        "# from utils import utils\n",
        "# from models.model import RawNet\n",
        "# from loss import GE2ELoss\n",
        "\n",
        "\n",
        "\n",
        "# torch.multiprocessing.set_sharing_strategy('file_system')"
      ],
      "metadata": {
        "id": "g7iQJrx4uvFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Loading**"
      ],
      "metadata": {
        "id": "3r77GR9iu2Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlnKK2hBu4PH",
        "outputId": "02e5b5fc-2ce7-4630-9fff-fd95e53a37c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_data(dataset_path, json_path):\n",
        "\n",
        "    # dictionary to store mapping, labels, and MFCCs\n",
        "    data = {\n",
        "        \"file_path\": [],\n",
        "        \"file_label_name\": [],\n",
        "        \"file_label_code\": [],\n",
        "    }\n",
        "\n",
        "    # loop through all language sub-folder\n",
        "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "        n = 1 # to keep track of ETA\n",
        "        # ensure we're processing a language sub-folder level\n",
        "        if dirpath is not dataset_path:\n",
        "\n",
        "            # save language label (i.e., sub-folder name) in the mapping\n",
        "            folder = dirpath.split(\"/\")[-1]\n",
        "            print(\"\\nProcessing: {}\".format(folder))\n",
        "\n",
        "            # process all audio files in sub-dir\n",
        "            for f in filenames:\n",
        "                # load audio file\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "                data[\"file_path\"].append(file_path)\n",
        "                data[\"file_label_name\"].append(folder)\n",
        "                data[\"file_label_code\"].append(i - 1)\n",
        "                print(str(n) + \" \" + file_path)\n",
        "                n += 1\n",
        "\n",
        "    # save list of files to json file\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(data, fp, indent=4)"
      ],
      "metadata": {
        "id": "Y7sdJiMJu9Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_data(DATASET_PATH, JSON_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHwmXhDpu86Q",
        "outputId": "20059f38-6fc9-47b0-bc6b-ebf8a8711561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: french\n",
            "1 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french10.wav\n",
            "2 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french3.wav\n",
            "3 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french2.wav\n",
            "4 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french13.wav\n",
            "5 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french11.wav\n",
            "6 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french14.wav\n",
            "7 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french5.wav\n",
            "8 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french1.wav\n",
            "9 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french4.wav\n",
            "10 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french12.wav\n",
            "11 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french7.wav\n",
            "12 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french8.wav\n",
            "13 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french15.wav\n",
            "14 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french6.wav\n",
            "15 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french9.wav\n",
            "16 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french17.wav\n",
            "17 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french26.wav\n",
            "18 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french28.wav\n",
            "19 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french25.wav\n",
            "20 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french16.wav\n",
            "21 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french32.wav\n",
            "22 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french24.wav\n",
            "23 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french33.wav\n",
            "24 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french23.wav\n",
            "25 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french21.wav\n",
            "26 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french18.wav\n",
            "27 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french27.wav\n",
            "28 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french20.wav\n",
            "29 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french22.wav\n",
            "30 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french31.wav\n",
            "31 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french30.wav\n",
            "32 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french19.wav\n",
            "33 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french29.wav\n",
            "34 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french49.wav\n",
            "35 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french34.wav\n",
            "36 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french47.wav\n",
            "37 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french45.wav\n",
            "38 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french50.wav\n",
            "39 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french37.wav\n",
            "40 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french41.wav\n",
            "41 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french36.wav\n",
            "42 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french38.wav\n",
            "43 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french39.wav\n",
            "44 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french44.wav\n",
            "45 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french43.wav\n",
            "46 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french48.wav\n",
            "47 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french40.wav\n",
            "48 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french46.wav\n",
            "49 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french35.wav\n",
            "50 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french42.wav\n",
            "51 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french66.wav\n",
            "52 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french58.wav\n",
            "53 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french52.wav\n",
            "54 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french63.wav\n",
            "55 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french56.wav\n",
            "56 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french51.wav\n",
            "57 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french62.wav\n",
            "58 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french55.wav\n",
            "59 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french57.wav\n",
            "60 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french54.wav\n",
            "61 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french65.wav\n",
            "62 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french53.wav\n",
            "63 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french64.wav\n",
            "64 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french60.wav\n",
            "65 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french67.wav\n",
            "66 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french68.wav\n",
            "67 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french61.wav\n",
            "68 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french59.wav\n",
            "69 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french78.wav\n",
            "70 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french77.wav\n",
            "71 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french74.wav\n",
            "72 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french75.wav\n",
            "73 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french73.wav\n",
            "74 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french70.wav\n",
            "75 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french76.wav\n",
            "76 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french81.wav\n",
            "77 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french84.wav\n",
            "78 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french79.wav\n",
            "79 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french82.wav\n",
            "80 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french71.wav\n",
            "81 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french69.wav\n",
            "82 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french72.wav\n",
            "83 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french80.wav\n",
            "84 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french83.wav\n",
            "85 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french85.wav\n",
            "86 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french93.wav\n",
            "87 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french86.wav\n",
            "88 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french99.wav\n",
            "89 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french96.wav\n",
            "90 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french95.wav\n",
            "91 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french90.wav\n",
            "92 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french87.wav\n",
            "93 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french97.wav\n",
            "94 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french91.wav\n",
            "95 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french100.wav\n",
            "96 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french94.wav\n",
            "97 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french88.wav\n",
            "98 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french89.wav\n",
            "99 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french92.wav\n",
            "100 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/french/french98.wav\n",
            "\n",
            "Processing: italian\n",
            "1 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian14.wav\n",
            "2 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian13.wav\n",
            "3 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian12.wav\n",
            "4 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian11.wav\n",
            "5 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian15.wav\n",
            "6 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian10.wav\n",
            "7 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian22.wav\n",
            "8 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian18.wav\n",
            "9 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian25.wav\n",
            "10 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian31.wav\n",
            "11 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian23.wav\n",
            "12 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian28.wav\n",
            "13 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian20.wav\n",
            "14 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian17.wav\n",
            "15 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian19.wav\n",
            "16 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian16.wav\n",
            "17 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian24.wav\n",
            "18 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian30.wav\n",
            "19 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian26.wav\n",
            "20 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian21.wav\n",
            "21 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian27.wav\n",
            "22 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian29.wav\n",
            "23 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian45.wav\n",
            "24 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian44.wav\n",
            "25 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian41.wav\n",
            "26 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian35.wav\n",
            "27 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian42.wav\n",
            "28 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian46.wav\n",
            "29 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian36.wav\n",
            "30 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian34.wav\n",
            "31 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian37.wav\n",
            "32 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian33.wav\n",
            "33 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian38.wav\n",
            "34 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian32.wav\n",
            "35 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian47.wav\n",
            "36 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian40.wav\n",
            "37 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian39.wav\n",
            "38 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian43.wav\n",
            "39 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian48.wav\n",
            "40 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian63.wav\n",
            "41 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian50.wav\n",
            "42 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian51.wav\n",
            "43 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian52.wav\n",
            "44 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian60.wav\n",
            "45 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian62.wav\n",
            "46 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian59.wav\n",
            "47 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian49.wav\n",
            "48 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian61.wav\n",
            "49 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian58.wav\n",
            "50 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian53.wav\n",
            "51 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian54.wav\n",
            "52 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian57.wav\n",
            "53 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian55.wav\n",
            "54 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian56.wav\n",
            "55 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian72.wav\n",
            "56 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian74.wav\n",
            "57 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian69.wav\n",
            "58 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian77.wav\n",
            "59 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian65.wav\n",
            "60 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian68.wav\n",
            "61 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian64.wav\n",
            "62 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian78.wav\n",
            "63 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian76.wav\n",
            "64 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian80.wav\n",
            "65 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian75.wav\n",
            "66 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian79.wav\n",
            "67 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian66.wav\n",
            "68 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian70.wav\n",
            "69 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian71.wav\n",
            "70 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian73.wav\n",
            "71 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian67.wav\n",
            "72 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian81.wav\n",
            "73 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian87.wav\n",
            "74 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian89.wav\n",
            "75 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian94.wav\n",
            "76 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian91.wav\n",
            "77 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian83.wav\n",
            "78 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian96.wav\n",
            "79 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian85.wav\n",
            "80 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian88.wav\n",
            "81 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian90.wav\n",
            "82 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian82.wav\n",
            "83 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian93.wav\n",
            "84 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian92.wav\n",
            "85 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian86.wav\n",
            "86 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian95.wav\n",
            "87 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian84.wav\n",
            "88 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian97.wav\n",
            "89 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian100.wav\n",
            "90 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian98.wav\n",
            "91 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian99.wav\n",
            "92 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian3.wav\n",
            "93 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian9.wav\n",
            "94 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian5.wav\n",
            "95 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian7.wav\n",
            "96 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian1.wav\n",
            "97 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian6.wav\n",
            "98 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian8.wav\n",
            "99 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian4.wav\n",
            "100 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/italian/italian2.wav\n",
            "\n",
            "Processing: spanish\n",
            "1 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish5.wav\n",
            "2 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish2.wav\n",
            "3 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish3.wav\n",
            "4 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish1.wav\n",
            "5 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish4.wav\n",
            "6 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish6.wav\n",
            "7 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish21.wav\n",
            "8 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish10.wav\n",
            "9 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish12.wav\n",
            "10 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish7.wav\n",
            "11 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish20.wav\n",
            "12 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish9.wav\n",
            "13 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish13.wav\n",
            "14 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish17.wav\n",
            "15 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish18.wav\n",
            "16 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish22.wav\n",
            "17 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish11.wav\n",
            "18 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish8.wav\n",
            "19 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish19.wav\n",
            "20 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish15.wav\n",
            "21 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish16.wav\n",
            "22 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish14.wav\n",
            "23 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish23.wav\n",
            "24 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish36.wav\n",
            "25 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish24.wav\n",
            "26 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish37.wav\n",
            "27 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish26.wav\n",
            "28 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish25.wav\n",
            "29 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish31.wav\n",
            "30 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish28.wav\n",
            "31 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish40.wav\n",
            "32 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish38.wav\n",
            "33 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish39.wav\n",
            "34 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish35.wav\n",
            "35 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish30.wav\n",
            "36 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish33.wav\n",
            "37 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish29.wav\n",
            "38 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish27.wav\n",
            "39 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish34.wav\n",
            "40 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish32.wav\n",
            "41 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish44.wav\n",
            "42 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish54.wav\n",
            "43 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish41.wav\n",
            "44 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish50.wav\n",
            "45 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish53.wav\n",
            "46 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish51.wav\n",
            "47 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish43.wav\n",
            "48 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish45.wav\n",
            "49 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish46.wav\n",
            "50 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish48.wav\n",
            "51 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish47.wav\n",
            "52 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish49.wav\n",
            "53 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish42.wav\n",
            "54 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish55.wav\n",
            "55 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish52.wav\n",
            "56 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish70.wav\n",
            "57 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish63.wav\n",
            "58 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish71.wav\n",
            "59 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish64.wav\n",
            "60 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish56.wav\n",
            "61 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish67.wav\n",
            "62 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish62.wav\n",
            "63 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish59.wav\n",
            "64 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish57.wav\n",
            "65 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish61.wav\n",
            "66 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish66.wav\n",
            "67 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish60.wav\n",
            "68 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish58.wav\n",
            "69 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish72.wav\n",
            "70 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish68.wav\n",
            "71 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish65.wav\n",
            "72 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish69.wav\n",
            "73 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish85.wav\n",
            "74 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish75.wav\n",
            "75 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish76.wav\n",
            "76 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish78.wav\n",
            "77 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish83.wav\n",
            "78 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish81.wav\n",
            "79 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish84.wav\n",
            "80 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish79.wav\n",
            "81 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish74.wav\n",
            "82 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish77.wav\n",
            "83 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish80.wav\n",
            "84 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish82.wav\n",
            "85 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish73.wav\n",
            "86 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish86.wav\n",
            "87 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish87.wav\n",
            "88 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish97.wav\n",
            "89 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish100.wav\n",
            "90 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish94.wav\n",
            "91 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish96.wav\n",
            "92 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish92.wav\n",
            "93 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish95.wav\n",
            "94 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish99.wav\n",
            "95 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish88.wav\n",
            "96 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish90.wav\n",
            "97 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish93.wav\n",
            "98 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish91.wav\n",
            "99 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish89.wav\n",
            "100 /content/drive/MyDrive/ma_dataset/similar_langs_small/100_set/spanish/spanish98.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train, Test, Validation sets**"
      ],
      "metadata": {
        "id": "Qi2lm6J82cJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path):\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    # convert lists to numpy arrays\n",
        "    paths = np.array(data[\"file_path\"])\n",
        "    labels = np.array(data[\"file_label_code\"])\n",
        "\n",
        "    return paths, labels"
      ],
      "metadata": {
        "id": "aLh3Gfic2n33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "paths, labels = load_data(JSON_PATH)"
      ],
      "metadata": {
        "id": "V-BXfTLU3U3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stratify-split the data into val(20%), train, test\n",
        "data_pre, data_val, labels_pre, labels_val = train_test_split(paths, labels, test_size=0.20, stratify= labels, random_state=42)\n",
        "data_train, data_test, labels_train, labels_test = train_test_split(data_pre, labels_pre, test_size=0.20, stratify= labels_pre, random_state=42)"
      ],
      "metadata": {
        "id": "quoNUzfS4e2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sMBcNAyLJWEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ready to use\n",
        "# data_train, labels_train\n",
        "# data_test, labels_test\n",
        "# data_val, labels_val"
      ],
      "metadata": {
        "id": "UPZiDH1u7LWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numpy_data_to_txt(X, y, output_file):\n",
        "  data = list(zip(X,y))\n",
        "  f = open(output_file,\"w+\") \n",
        "  for element in data: #you wouldn't need to write this since you are already in a loop\n",
        "    f.write(f\"{element[0]} {element[1]} \\n\") \n",
        "  f.close()\n"
      ],
      "metadata": {
        "id": "ai2aLP-IAlVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_data_to_txt(data_train,labels_train, OUTPUT_LOC+\"training.txt\")\n",
        "numpy_data_to_txt(data_test,labels_test, OUTPUT_LOC+\"testing.txt\")\n",
        "numpy_data_to_txt(data_val,labels_val, OUTPUT_LOC+\"validation.txt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ss6SNYMP-evr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature extraction**"
      ],
      "metadata": {
        "id": "eQaMCA1o75Zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**helper functions**\n",
        "\n"
      ],
      "metadata": {
        "id": "XK3MQobz8X0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_wav(audio_filepath, sr, min_dur_sec=4):\n",
        "    audio_data,fs  = librosa.load(audio_filepath,sr=16000)\n",
        "    len_file = len(audio_data)\n",
        "    \n",
        "    if len_file <int(min_dur_sec*sr):\n",
        "        dummy=np.zeros((1,int(min_dur_sec*sr)-len_file))\n",
        "        extened_wav = np.concatenate((audio_data,dummy[0]))\n",
        "    else:\n",
        "        \n",
        "        extened_wav = audio_data\n",
        "    return extened_wav\n",
        "\n",
        "\n",
        "def lin_mel_from_wav(wav, hop_length, win_length, n_mels):\n",
        "    linear = librosa.feature.melspectrogram(wav, n_mels=n_mels, win_length=win_length, hop_length=hop_length) # linear spectrogram\n",
        "    return linear.T\n",
        "\n",
        "def lin_spectogram_from_wav(wav, hop_length, win_length, n_fft=512):\n",
        "    linear = librosa.stft(wav, n_fft=n_fft, win_length=win_length, hop_length=hop_length) # linear spectrogram\n",
        "    return linear.T\n",
        "\n",
        "\n",
        "def feature_extraction(filepath,sr=16000, min_dur_sec=4,win_length=400,hop_length=160, n_mels=40, spec_len=400,mode='train'):\n",
        "    audio_data = load_wav(filepath, sr=sr,min_dur_sec=min_dur_sec)\n",
        "    linear_spect = lin_spectogram_from_wav(audio_data, hop_length, win_length, n_fft=512)\n",
        "    mag, _ = librosa.magphase(linear_spect)  # magnitude\n",
        "    mag_T = mag.T\n",
        "    mu = np.mean(mag_T, 0, keepdims=True)\n",
        "    std = np.std(mag_T, 0, keepdims=True)\n",
        "    return (mag_T - mu) / (std + 1e-5)\n",
        "    \n",
        "\n",
        "def load_raw_data(filepath,sr=16000):\n",
        "    audio_data = load_wav(filepath, sr=sr,min_dur_sec=3.0)\n",
        "    \n",
        "    \n",
        "def load_data(filepath,sr=16000, min_dur_sec=4,win_length=400,hop_length=160, n_mels=40, spec_len=400,mode='train'):\n",
        "    audio_data = load_wav(filepath, sr=sr,min_dur_sec=min_dur_sec)\n",
        "    #linear_spect = lin_spectogram_from_wav(audio_data, hop_length, win_length, n_mels)\n",
        "    linear_spect = lin_spectogram_from_wav(audio_data, hop_length, win_length, n_fft=512)\n",
        "    mag, _ = librosa.magphase(linear_spect)  # magnitude\n",
        "    mag_T = mag.T\n",
        "    \n",
        "\n",
        "    if mode=='train':\n",
        "        randtime = np.random.randint(0, mag_T.shape[1]-spec_len)\n",
        "        spec_mag = mag_T[:, randtime:randtime+spec_len]\n",
        "    else:\n",
        "        spec_mag = mag_T\n",
        "    \n",
        "    # preprocessing, subtract mean, divided by time-wise var\n",
        "    mu = np.mean(spec_mag, 0, keepdims=True)\n",
        "    std = np.std(spec_mag, 0, keepdims=True)\n",
        "    return (spec_mag - mu) / (std + 1e-5)\n",
        "    \n",
        "\n",
        "\n",
        "def load_npy_data(filepath,spec_len=400,mode='train'):\n",
        "    mag_T = np.load(filepath)\n",
        "    if mode=='train':\n",
        "        randtime = np.random.randint(0, mag_T.shape[1]-spec_len)\n",
        "        spec_mag = mag_T[:, randtime:randtime+spec_len]\n",
        "    else:\n",
        "        spec_mag = mag_T\n",
        "    return spec_mag\n",
        "    \n",
        "\n",
        "\n",
        "def speech_collate(batch):\n",
        "    targets = []\n",
        "    specs = []\n",
        "    for sample in batch:\n",
        "        specs.append(sample['features'])\n",
        "        targets.append((sample['labels']))\n",
        "    return specs, targets\n"
      ],
      "metadata": {
        "id": "IxT4va-K8XcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_features(audio_filepath):\n",
        "    features = feature_extraction(audio_filepath)\n",
        "    return features\n",
        "    \n",
        "    \n",
        "\n",
        "def FE_pipeline(feature_list,store_loc,mode):\n",
        "    create_root = os.path.join(store_loc,mode)\n",
        "    if not os.path.exists(create_root):\n",
        "        os.makedirs(create_root)\n",
        "    if mode=='train':\n",
        "        fid = open(OUTPUT_LOC+'training_feat.txt','w')\n",
        "    elif mode=='test':\n",
        "        fid = open(OUTPUT_LOC+'testing_feat.txt','w')\n",
        "    elif mode=='validation':\n",
        "        fid = open(OUTPUT_LOC+'validation_feat.txt','w')\n",
        "    else:\n",
        "        print('Unknown mode')\n",
        "    \n",
        "    for row in feature_list:\n",
        "        filepath = row.split(' ')[0]\n",
        "        lang_id = row.split(' ')[1]\n",
        "        vid_folder = filepath.split('/')[-2]\n",
        "        lang_folder = filepath.split('/')[-3]\n",
        "        filename = filepath.split('/')[-1]\n",
        "        create_folders = os.path.join(create_root,lang_folder,vid_folder)\n",
        "        if not os.path.exists(create_folders):\n",
        "            os.makedirs(create_folders)\n",
        "        extract_feats = extract_features(filepath)\n",
        "        dest_filepath = create_folders+'/'+filename[:-4]+'.npy'\n",
        "        np.save(dest_filepath,extract_feats)\n",
        "        to_write = dest_filepath+' '+lang_id\n",
        "        fid.write(to_write+'\\n')\n",
        "    fid.close()\n",
        "    "
      ],
      "metadata": {
        "id": "LDLaDt5I8XXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_loc = OUTPUT_LOC+'features'\n",
        "read_train = [line.rstrip('\\n') for line in open(OUTPUT_LOC+'training.txt')]\n",
        "FE_pipeline(read_train,store_loc,mode='train')\n",
        "\n",
        "read_test = [line.rstrip('\\n') for line in open(OUTPUT_LOC+'testing.txt')]\n",
        "FE_pipeline(read_test,store_loc,mode='test')\n",
        "\n",
        "read_val = [line.rstrip('\\n') for line in open(OUTPUT_LOC+'validation.txt')]\n",
        "FE_pipeline(read_val,store_loc,mode='validation')\n",
        "    "
      ],
      "metadata": {
        "id": "2YiWYDyd8DH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**dataloaders**"
      ],
      "metadata": {
        "id": "8S0xmMQVG8Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DatasetLoader_D():\n",
        "    \"\"\"Speech dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, manifest, mode):\n",
        "        \"\"\"\n",
        "        Read the textfile and get the paths\n",
        "        \"\"\"\n",
        "        self.mode=mode\n",
        "        self.audio_links = [line.rstrip('\\n').split(' ')[0] for line in open(manifest)]\n",
        "        self.labels = [int(line.rstrip('\\n').split(' ')[1]) for line in open(manifest)]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_links)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_link =self.audio_links[idx]\n",
        "        class_id = self.labels[idx]\n",
        "        ### select M random files\n",
        "        get_ids = [el==class_id for el in self.labels]\n",
        "        get_all_files = list(compress(self.audio_links, get_ids))\n",
        "        selected_files = get_all_files\n",
        "        specs = []\n",
        "        labels_list = []\n",
        "        for audio_filepath in selected_files:\n",
        "            spec = load_data(audio_link,mode=self.mode)\n",
        "            specs.append(spec)\n",
        "            labels_list.append(class_id)\n",
        "        feats = np.asarray(specs)\n",
        "        label_arr = np.asarray(labels_list)\n",
        "        # print(\"here\",len(torch.from_numpy(np.ascontiguousarray(feats))))\n",
        "        \n",
        "        sample = {'features': torch.from_numpy(np.ascontiguousarray(feats)), 'labels': torch.from_numpy(np.ascontiguousarray(label_arr))}\n",
        "        return sample\n",
        "        \n",
        "    "
      ],
      "metadata": {
        "id": "XGwOJf8UG_s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class DatasetLoader_X():\n",
        "    \"\"\"Speech dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, manifest, mode):\n",
        "        \"\"\"\n",
        "        Read the textfile and get the paths\n",
        "        \"\"\"\n",
        "        self.mode=mode\n",
        "        self.audio_links = [line.rstrip('\\n').split(' ')[0] for line in open(manifest)]\n",
        "        self.labels = [int(line.rstrip('\\n').split(' ')[1]) for line in open(manifest)]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_links)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_link =self.audio_links[idx]\n",
        "        class_id = self.labels[idx]\n",
        "        #lang_label=lang_id[self.audio_links[idx].split('/')[-2]]\n",
        "        spec = load_data(audio_link,mode=self.mode)\n",
        "        sample = {'features': torch.from_numpy(np.ascontiguousarray(spec)), 'labels': torch.from_numpy(np.ascontiguousarray(class_id))}\n",
        "        return sample\n",
        "        "
      ],
      "metadata": {
        "id": "uKToUYPkDRr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**D-Vector Architecture**"
      ],
      "metadata": {
        "id": "hwf1PdmYDFe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d-vector\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock3x3(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, inplanes3, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock3x3, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes3, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.01,inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RawNet(nn.Module):\n",
        "    def __init__(self, input_channel, num_classes=1211):\n",
        "        self.inplanes3 = 128\n",
        "        super(RawNet, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channel, 128, kernel_size=3, stride=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.01,inplace=True)\n",
        "        #############################################################################\n",
        "        \n",
        "        \n",
        "        self.resblock_1_1 = self._make_layer3(BasicBlock3x3, 128, 1, stride=1)\n",
        "        self.resblock_1_2 = self._make_layer3(BasicBlock3x3, 128, 1, stride=1)\n",
        "        self.maxpool_resblock_1 = nn.MaxPool1d(kernel_size=3, stride=1, padding=0)\n",
        "        #############################################################################\n",
        "        self.resblock_2_1 = self._make_layer3(BasicBlock3x3, 256, 1, stride=1)\n",
        "        self.resblock_2_2 = self._make_layer3(BasicBlock3x3, 256, 1, stride=1)\n",
        "        self.resblock_2_3 = self._make_layer3(BasicBlock3x3, 256, 1, stride=1)\n",
        "        self.resblock_2_4 = self._make_layer3(BasicBlock3x3, 256, 1, stride=1)\n",
        "        self.maxpool_resblock_2 = nn.MaxPool1d(kernel_size=3, stride=2, padding=0)\n",
        "        \n",
        "        ############################################################################\n",
        "        self.gru = nn.GRU(input_size=256, hidden_size=1024,dropout=0.2,bidirectional=False,batch_first=True)\n",
        "        self.spk_emb = nn.Linear(1024,128)\n",
        "        # self.drop = nn.Dropout(p=0.2)\n",
        "        self.output_layer = nn.Linear(128, num_classes)\n",
        "\n",
        "\n",
        "    def _make_layer3(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        # print(f\"Make layers: {planes}, {block.expansion}\")\n",
        "        if stride != 1 or self.inplanes3 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes3, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes3, planes, stride, downsample))\n",
        "        self.inplanes3 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes3, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        " \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        out = self.conv1(inputs)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        ## ResBlock-1\n",
        "        out = self.resblock_1_1(out)\n",
        "        out = self.maxpool_resblock_1(out)\n",
        "        out = self.resblock_1_2(out)\n",
        "        out = self.maxpool_resblock_1(out)\n",
        "        ##Resblock-2\n",
        "        out = self.resblock_2_1(out)\n",
        "        out = self.maxpool_resblock_2(out)\n",
        "        out = self.resblock_2_2(out)\n",
        "        out = self.maxpool_resblock_2(out)\n",
        "        out = self.resblock_2_3(out)\n",
        "        out = self.maxpool_resblock_2(out)\n",
        "        out = self.resblock_2_4(out)\n",
        "        out = self.maxpool_resblock_2(out)\n",
        "        ### GRU\n",
        "        out = out.permute(0,2,1)\n",
        "        out,_ = self.gru(out)\n",
        "        out = out.permute(0,2,1)\n",
        "        spk_embeddings = self.spk_emb(out[:,:,-1])\n",
        "        preds = self.output_layer(spk_embeddings)\n",
        "\n",
        "        return preds,spk_embeddings"
      ],
      "metadata": {
        "id": "P5AN5JnPNqJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**X-Vector Architecture**"
      ],
      "metadata": {
        "id": "3md6QhWgIuwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x-vector\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TDNN(nn.Module):\n",
        "    \n",
        "    def __init__(\n",
        "                    self, \n",
        "                    input_dim=23, \n",
        "                    output_dim=512,\n",
        "                    context_size=5,\n",
        "                    stride=1,\n",
        "                    dilation=1,\n",
        "                    batch_norm=False,\n",
        "                    dropout_p=0.2\n",
        "                ):\n",
        "        '''\n",
        "        TDNN as defined by https://www.danielpovey.com/files/2015_interspeech_multisplice.pdf\n",
        "        Affine transformation not applied globally to all frames but smaller windows with local context\n",
        "        batch_norm: True to include batch normalisation after the non linearity\n",
        "        \n",
        "        Context size and dilation determine the frames selected\n",
        "        (although context size is not really defined in the traditional sense)\n",
        "        For example:\n",
        "            context size 5 and dilation 1 is equivalent to [-2,-1,0,1,2]\n",
        "            context size 3 and dilation 2 is equivalent to [-2, 0, 2]\n",
        "            context size 1 and dilation 1 is equivalent to [0]\n",
        "        '''\n",
        "        super(TDNN, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.stride = stride\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dilation = dilation\n",
        "        self.dropout_p = dropout_p\n",
        "        self.batch_norm = batch_norm\n",
        "      \n",
        "        self.kernel = nn.Linear(input_dim*context_size, output_dim)\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "        if self.batch_norm:\n",
        "            self.bn = nn.BatchNorm1d(output_dim)\n",
        "        if self.dropout_p:\n",
        "            self.drop = nn.Dropout(p=self.dropout_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        input: size (batch, seq_len, input_features)\n",
        "        outpu: size (batch, new_seq_len, output_features)\n",
        "        '''\n",
        "        \n",
        "        _, _, d = x.shape\n",
        "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Unfold input into smaller temporal contexts\n",
        "        x = F.unfold(\n",
        "                        x, \n",
        "                        (self.context_size, self.input_dim), \n",
        "                        stride=(1,self.input_dim), \n",
        "                        dilation=(self.dilation,1)\n",
        "                    )\n",
        "\n",
        "        # N, output_dim*context_size, new_t = x.shape\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.kernel(x.float())\n",
        "        x = self.nonlinearity(x)\n",
        "        \n",
        "        if self.dropout_p:\n",
        "            x = self.drop(x)\n",
        "\n",
        "        if self.batch_norm:\n",
        "            x = x.transpose(1,2)\n",
        "            x = self.bn(x)\n",
        "            x = x.transpose(1,2)\n",
        "\n",
        "        return x\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class X_vector(nn.Module):\n",
        "    def __init__(self, input_dim = 40, num_classes=8):\n",
        "        super(X_vector, self).__init__()\n",
        "        self.tdnn1 = TDNN(input_dim=input_dim, output_dim=512, context_size=5, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn2 = TDNN(input_dim=512, output_dim=512, context_size=3, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn3 = TDNN(input_dim=512, output_dim=512, context_size=2, dilation=2,dropout_p=0.5)\n",
        "        self.tdnn4 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn5 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=3,dropout_p=0.5)\n",
        "        #### Frame levelPooling\n",
        "        self.segment6 = nn.Linear(1024, 512)\n",
        "        self.segment7 = nn.Linear(512, 512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    def forward(self, inputs):\n",
        "        tdnn1_out = self.tdnn1(inputs)\n",
        "        tdnn2_out = self.tdnn2(tdnn1_out)\n",
        "        tdnn3_out = self.tdnn3(tdnn2_out)\n",
        "        tdnn4_out = self.tdnn4(tdnn3_out)\n",
        "        tdnn5_out = self.tdnn5(tdnn4_out)\n",
        "        ### Stat Pool\n",
        "        \n",
        "        mean = torch.mean(tdnn5_out,1)\n",
        "        std = torch.var(tdnn5_out,1)\n",
        "        stat_pooling = torch.cat((mean,std),1)\n",
        "        segment6_out = self.segment6(stat_pooling)\n",
        "        x_vec = self.segment7(segment6_out)\n",
        "        predictions = self.output(x_vec)\n",
        "        return predictions,x_vec"
      ],
      "metadata": {
        "id": "8uoVPcwb_hbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**training and validation functions** "
      ],
      "metadata": {
        "id": "wk8OVK4vKemG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def train(dataloader_train,epoch, model, device, optimizer,ce_loss,  mode=\"x\"):\n",
        "    train_loss_list=[]\n",
        "    full_preds=[]\n",
        "    full_gts=[]\n",
        "    model.train()\n",
        "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
        "        if mode == \"x\":\n",
        "          features = torch.from_numpy(np.asarray([torch_tensor.numpy().T for torch_tensor in sample_batched[0]])).float()\n",
        "          labels = torch.from_numpy(np.asarray([torch_tensor[0].numpy() for torch_tensor in sample_batched[1]]))\n",
        "        else:\n",
        "          features = torch.cat((sample_batched[0])).float()\n",
        "          labels = torch.cat((sample_batched[1]))\n",
        "        features, labels = features.to(device),labels.to(device)\n",
        "        features.requires_grad = True\n",
        "        optimizer.zero_grad()\n",
        "        pred_logits,vec = model(features)\n",
        "        #### CE loss\n",
        "        loss = ce_loss(pred_logits,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_list.append(loss.item())\n",
        "        #train_acc_list.append(accuracy)\n",
        "        #if i_batch%10==0:\n",
        "        #    print('Loss {} after {} iteration'.format(np.mean(np.asarray(train_loss_list)),i_batch))\n",
        "        \n",
        "        predictions = np.argmax(pred_logits.detach().cpu().numpy(),axis=1)\n",
        "        for pred in predictions:\n",
        "            full_preds.append(pred)\n",
        "        for lab in labels.detach().cpu().numpy():\n",
        "            full_gts.append(lab)\n",
        "            \n",
        "    mean_acc = accuracy_score(full_gts,full_preds)\n",
        "    mean_loss = np.mean(np.asarray(train_loss_list))\n",
        "    print('Total training loss {} and training Accuracy {} after {} epochs'.format(mean_loss,mean_acc,epoch))\n",
        "    \n",
        "\n",
        "\n",
        "def validation(dataloader_val,epoch, model, device, optimizer,ce_loss,  mode=\"x\"):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss_list=[]\n",
        "        full_preds=[]\n",
        "        full_gts=[]\n",
        "        for i_batch, sample_batched in enumerate(dataloader_val):\n",
        "            if mode == \"x\":\n",
        "              features = torch.from_numpy(np.asarray([torch_tensor.numpy().T for torch_tensor in sample_batched[0]])).float()\n",
        "              labels = torch.from_numpy(np.asarray([torch_tensor[0].numpy() for torch_tensor in sample_batched[1]]))\n",
        "            else:\n",
        "              features = torch.cat((sample_batched[0])).float()\n",
        "              labels = torch.cat((sample_batched[1]))\n",
        "            features, labels = features.to(device),labels.to(device)\n",
        "            pred_logits,vec = model(features)\n",
        "            #### CE loss\n",
        "            loss = ce_loss(pred_logits,labels)\n",
        "            val_loss_list.append(loss.item())\n",
        "            #train_acc_list.append(accuracy)\n",
        "            predictions = np.argmax(pred_logits.detach().cpu().numpy(),axis=1)\n",
        "            for pred in predictions:\n",
        "                full_preds.append(pred)\n",
        "            for lab in labels.detach().cpu().numpy():\n",
        "                full_gts.append(lab)\n",
        "                \n",
        "        mean_acc = accuracy_score(full_gts,full_preds)\n",
        "        mean_loss = np.mean(np.asarray(val_loss_list))\n",
        "        print('Total vlidation loss {} and Validation accuracy {} after {} epochs'.format(mean_loss,mean_acc,epoch))\n",
        "        \n",
        "        model_save_path = os.path.join(OUTPUT_LOC+'save_model', 'best_check_point_'+str(epoch)+'_'+str(mean_loss))\n",
        "        state_dict = {'model': model.state_dict(),'optimizer': optimizer.state_dict(),'epoch': epoch}\n",
        "        torch.save(state_dict, model_save_path)"
      ],
      "metadata": {
        "id": "s1KaXBIhcCqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3uGpAeShNjCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**driver code**"
      ],
      "metadata": {
        "id": "UzMmBLUaKk7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_train(mode=\"x\", languages = 6):\n",
        "    ### Data related\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "      \n",
        "    if mode == \"x\":\n",
        "      dataset_train = DatasetLoader_X(manifest=OUTPUT_LOC+'training.txt',mode='train')\n",
        "      dataloader_train = DataLoader(dataset_train, batch_size=4,shuffle=True,collate_fn=speech_collate) \n",
        "      dataset_val = DatasetLoader_X(manifest=OUTPUT_LOC+'validation.txt',mode='train')\n",
        "      dataloader_val = DataLoader(dataset_val, batch_size=4,shuffle=True,collate_fn=speech_collate) \n",
        "      model = X_vector(257, languages)\n",
        "    else:\n",
        "      dataset_train = DatasetLoader_D(manifest=OUTPUT_LOC+'training.txt',mode='train')\n",
        "      dataloader_train = DataLoader(dataset_train, batch_size=4,shuffle=True,collate_fn=speech_collate) \n",
        "      dataset_val = DatasetLoader_D(manifest=OUTPUT_LOC+'validation.txt',mode='train')\n",
        "      dataloader_val = DataLoader(dataset_val, batch_size=4,shuffle=True,collate_fn=speech_collate) \n",
        "      model = RawNet(257 ,languages)\n",
        "      \n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0, betas=(0.9, 0.98), eps=1e-9)   \n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    for epoch in tqdm(range(20)):\n",
        "      train(dataloader_train,epoch, model, device, optimizer, ce_loss,  mode)\n",
        "      validation(dataloader_val,epoch, model, device, optimizer,ce_loss,  mode)"
      ],
      "metadata": {
        "id": "Nmq_JOrYDa-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_train(mode=\"x\", languages = LANGUAGES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LXMxUukK1GX",
        "outputId": "5db1c319-b3cc-4a0a-ab5a-88cf88536c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 1.103741390009721 and training Accuracy 0.328125 after 0 epochs\n",
            "Total vlidation loss 1.0985203345616659 and Validation accuracy 0.3333333333333333 after 0 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 1/20 [00:33<10:31, 33.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 1.0981587022542953 and training Accuracy 0.296875 after 1 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [01:03<09:31, 31.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.0929180145263673 and Validation accuracy 0.2833333333333333 after 1 epochs\n",
            "Total training loss 1.0645711819330852 and training Accuracy 0.375 after 2 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [01:35<08:55, 31.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.0811744213104248 and Validation accuracy 0.43333333333333335 after 2 epochs\n",
            "Total training loss 1.0026376756529014 and training Accuracy 0.4270833333333333 after 3 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [02:06<08:22, 31.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.0288559516270956 and Validation accuracy 0.4666666666666667 after 3 epochs\n",
            "Total training loss 0.9270490060249964 and training Accuracy 0.5416666666666666 after 4 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [02:37<07:49, 31.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 0.9776626348495483 and Validation accuracy 0.5333333333333333 after 4 epochs\n",
            "Total training loss 0.8215190898627043 and training Accuracy 0.5572916666666666 after 5 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 6/20 [03:08<07:14, 31.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 0.9983925938606262 and Validation accuracy 0.5 after 5 epochs\n",
            "Total training loss 0.78577094959716 and training Accuracy 0.5104166666666666 after 6 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [03:38<06:42, 30.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 0.9754094084103903 and Validation accuracy 0.5833333333333334 after 6 epochs\n",
            "Total training loss 0.7424181327223778 and training Accuracy 0.6041666666666666 after 7 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [04:09<06:09, 30.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.0301928997039795 and Validation accuracy 0.5666666666666667 after 7 epochs\n",
            "Total training loss 0.7027506893500686 and training Accuracy 0.6822916666666666 after 8 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [04:41<05:42, 31.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 0.9100666999816894 and Validation accuracy 0.6 after 8 epochs\n",
            "Total training loss 0.6422528040905794 and training Accuracy 0.6822916666666666 after 9 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [05:11<05:09, 30.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 0.9605853408575058 and Validation accuracy 0.6166666666666667 after 9 epochs\n",
            "Total training loss 0.6205099048092961 and training Accuracy 0.6927083333333334 after 10 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [05:42<04:37, 30.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 0.9611064632733662 and Validation accuracy 0.5666666666666667 after 10 epochs\n",
            "Total training loss 0.6228311744829019 and training Accuracy 0.6927083333333334 after 11 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [06:12<04:06, 30.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 0.9068485021591186 and Validation accuracy 0.6 after 11 epochs\n",
            "Total training loss 0.527316767256707 and training Accuracy 0.796875 after 12 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [06:44<03:37, 31.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.6872923791408538 and Validation accuracy 0.5 after 12 epochs\n",
            "Total training loss 0.6061593654255072 and training Accuracy 0.734375 after 13 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [07:15<03:05, 30.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 0.9567465866605441 and Validation accuracy 0.5666666666666667 after 13 epochs\n",
            "Total training loss 0.4687606973069099 and training Accuracy 0.8229166666666666 after 14 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [07:45<02:34, 30.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.1951275726159414 and Validation accuracy 0.5666666666666667 after 14 epochs\n",
            "Total training loss 0.540039622845749 and training Accuracy 0.7395833333333334 after 15 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [08:16<02:03, 30.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 0.9776402354240418 and Validation accuracy 0.55 after 15 epochs\n",
            "Total training loss 0.4156379991521438 and training Accuracy 0.8229166666666666 after 16 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [08:48<01:33, 31.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.3798449700077375 and Validation accuracy 0.5666666666666667 after 16 epochs\n",
            "Total training loss 0.3762348381569609 and training Accuracy 0.859375 after 17 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [09:19<01:01, 30.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.135534823934237 and Validation accuracy 0.5833333333333334 after 17 epochs\n",
            "Total training loss 0.3865354358373831 and training Accuracy 0.8385416666666666 after 18 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 19/20 [09:49<00:30, 30.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.0312312444051106 and Validation accuracy 0.6333333333333333 after 18 epochs\n",
            "Total training loss 0.28545340164176497 and training Accuracy 0.8802083333333334 after 19 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [10:20<00:00, 31.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.183060358464718 and Validation accuracy 0.6 after 19 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_train(mode=\"d\", languages = LANGUAGES)"
      ],
      "metadata": {
        "id": "jw4spSZXK1C1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c19f6a4-1ed6-4e19-ed5e-7270df6987ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training loss 1.0710926577448845 and training Accuracy 0.4823404947916667 after 0 epochs\n",
            "Total vlidation loss 1.1973120411237081 and Validation accuracy 0.3641666666666667 after 0 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 1/20 [26:33<8:24:33, 1593.36s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training loss 0.8898469346264998 and training Accuracy 0.6216634114583334 after 1 epochs\n",
            "Total vlidation loss 1.1137486060460409 and Validation accuracy 0.42333333333333334 after 1 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 2/20 [52:49<7:55:03, 1583.51s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training loss 0.7266308528681596 and training Accuracy 0.69775390625 after 2 epochs\n",
            "Total vlidation loss 1.1037898321946462 and Validation accuracy 0.5308333333333334 after 2 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 3/20 [1:19:23<7:29:57, 1588.08s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training loss 0.5917930697711805 and training Accuracy 0.741455078125 after 3 epochs\n",
            "Total vlidation loss 0.8282718032598495 and Validation accuracy 0.6491666666666667 after 3 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 4/20 [1:46:01<7:04:34, 1592.18s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training loss 0.4387635923922062 and training Accuracy 0.8451334635416666 after 4 epochs\n",
            "Total vlidation loss 0.9018092423677444 and Validation accuracy 0.6333333333333333 after 4 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 5/20 [2:12:40<6:38:39, 1594.62s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training loss 0.42164528820042807 and training Accuracy 0.846923828125 after 5 epochs\n",
            "Total vlidation loss 1.0273851851622264 and Validation accuracy 0.6016666666666667 after 5 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 6/20 [2:39:25<6:12:51, 1597.94s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training loss 0.2582693822332658 and training Accuracy 0.9049479166666666 after 6 epochs\n",
            "Total vlidation loss 0.9956939200560252 and Validation accuracy 0.6008333333333333 after 6 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▌      | 7/20 [3:06:05<5:46:23, 1598.77s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training loss 0.24873027694411576 and training Accuracy 0.9235026041666666 after 7 epochs\n",
            "Total vlidation loss 1.3389460613330206 and Validation accuracy 0.5466666666666666 after 7 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 8/20 [3:32:46<5:19:53, 1599.45s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training loss 0.2290441335790092 and training Accuracy 0.89404296875 after 8 epochs\n",
            "Total vlidation loss 1.309056439002355 and Validation accuracy 0.595 after 8 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 45%|████▌     | 9/20 [3:59:28<4:53:23, 1600.31s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training loss 0.12881136843255567 and training Accuracy 0.9620768229166666 after 9 epochs\n",
            "Total vlidation loss 1.0536251078049341 and Validation accuracy 0.6475 after 9 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 10/20 [4:26:13<4:26:55, 1601.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 0.1387709007152201 and training Accuracy 0.9384765625 after 10 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [4:52:50<4:00:01, 1600.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.301651124904553 and Validation accuracy 0.63 after 10 epochs\n",
            "Total training loss 0.12046699700780057 and training Accuracy 0.95556640625 after 11 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [5:19:21<3:32:58, 1597.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vlidation loss 1.188602396969994 and Validation accuracy 0.645 after 11 epochs\n",
            "Total training loss 0.09969641213441112 and training Accuracy 0.9645182291666666 after 12 epochs\n",
            "Total vlidation loss 1.7637654994924863 and Validation accuracy 0.5841666666666666 after 12 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [5:45:50<3:06:04, 1594.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 0.14582442666505813 and training Accuracy 0.9534505208333334 after 13 epochs\n",
            "Total vlidation loss 1.4232726410031318 and Validation accuracy 0.645 after 13 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [6:12:24<2:39:27, 1594.52s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "# to do\n",
        "# lower the learning rate. it might be the problem\n",
        "# 1) Gradually decrease the learning rate to 0.0001. \n",
        "# 2) Add more data. \n",
        "# 3) Gradually increase the Dropout rates to ~0.2. Keep it consistent throughout the network. \n",
        "# 4) Decrease your batch size. \n",
        "# 5) Try a different optimizer (choose one that gives you the smallest loss).\n",
        "# chnage shuffle to false\n",
        "\n",
        "# source of the implementations:\n",
        "# https://github.com/KrishnaDN/d-vector-language-recognition\n",
        "# https://github.com/KrishnaDN/x-vector-pytorch\n",
        "# https://github.com/gzhu06/Y-vector\n"
      ],
      "metadata": {
        "id": "0PNY13R1kLQ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}